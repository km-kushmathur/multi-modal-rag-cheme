{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985474e9",
   "metadata": {},
   "source": [
    "# Multi-modal RAG for Chemical Engineers\n",
    "\n",
    "**Method:**\n",
    "* **Ingest a PDF**: Process a PDF of a chemical engineering textbook.\n",
    "* **Partition**: Using `unstructured.io`, break the PDF into its constituent parts: text, tables, and images.\n",
    "* **Summarize**: Generate concise summaries for every text chunk, table, and image using a LLMs.\n",
    "* **Index**: Summaries will be embedded with an embedding model (such as `text-embedding-3-small`) and stored in a `Chroma` vector store (`MultiVectorRetriever` strategy searches over small, dense summaries).\n",
    "* **Retrieve & Generate**: When a user asks a question, the system will retrieve the most relevant *original* text, tables, or images based on the summary search and use a multi-modal LLM (such as `gpt-4o-mini`) to generate a comprehensive answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba4c6a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632fb56",
   "metadata": {},
   "source": [
    "This notebook requires a specific setup, including a Linux environment (like WSL for Windows), system packages for document processing, and several Python libraries.\n",
    "\n",
    "### 1a. System-Level Dependencies (Run in Terminal)\n",
    "First, you need to install libraries for processing PDFs and images. Run the following command in your Ubuntu/WSL terminal:\n",
    "```bash\n",
    "sudo apt-get update && sudo apt-get install -y poppler-utils tesseract-ocr libmagic-dev\n",
    "```\n",
    "\n",
    "### 1b. Python Environment Setup (Run in Terminal)\n",
    "\n",
    "1. Create the virtual environment (only needs to be done once)\n",
    "    ```bash\n",
    "    python3 -m venv venv\n",
    "    ```\n",
    "\n",
    "2. Activate the environment (do this every time you start a new session)\n",
    "    ```bash\n",
    "    source ./venv/bin/activate\n",
    "    ```\n",
    "\n",
    "3. Install Jupyter and create a kernel for this environment\n",
    "    ```bash\n",
    "    pip install jupyter ipykernel\n",
    "    python -m ipykernel install --user --name=chem_rag_env --display-name \"Python (ChemE RAG)\"\n",
    "    ```\n",
    "\n",
    "4. Restart VS Code and select the \"Python (ChemE RAG)\" kernel.\n",
    "\n",
    "### 1c. Core Dependencies\n",
    "Run the following cell to install Unstructured, ChromaDB, LangChain, and Python Dotenv (for API keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"unstructured[all-docs]\" pillow lxml\n",
    "%pip install -U chromadb tiktoken\n",
    "%pip install -U langchain langchain-community langchain-openai langchain-groq\n",
    "%pip install -U python_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b5be7",
   "metadata": {},
   "source": [
    "## 2. API Key Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009f9af",
   "metadata": {},
   "source": [
    "This RAG system uses services from Azure OpenAI (for embeddings and generation), and Groq (for fast text summarization). `python-dotenv` is used to manage our API keys securely within the notebook.\n",
    "\n",
    "**Important**: The cell below looks for your API keys stored as environment variables. Make sure you have a `.env` file in the root directory of this project if you plan on sharing or showing this notebook to others to protect your API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67837690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"YOUR_MULTIMODAL_LLM_ENDPOINT\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"YOUR_MULTIMODAL_LLM_API_VERSION\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_MULTIMODAL_LLM_API_KEY\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"YOUR_MULTIMODAL_LLM_DEPLOYMENT_NAME\"\n",
    "os.environ[\"AZURE_EMBEDDING_ENDPOINT\"] = \"YOUR_EMBEDDING_MODEL_ENDPOINT\"\n",
    "os.environ[\"AZURE_EMBEDDING_API_KEY\"] = \"YOUR_EMBEDDING_MODEL_API_KEY\"\n",
    "os.environ[\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"] = \"YOUR_EMBEDDING_MODEL_DEPLOYMENT_NAME\"\n",
    "os.environ[\"EMBEDDING_API_VERSION\"] = \"YOUR_EMBEDDING_MODEL_API_VERSION\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\" # Set to \"true\" if you want to enable LangChain tracing (Debugging with LangChain)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07995435",
   "metadata": {},
   "source": [
    "## 3. Partitioning the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800a44b",
   "metadata": {},
   "source": [
    "### 3a. Using Unstructured\n",
    "Now that the environment is set up, you can parse your document using a PDF file.\n",
    "\n",
    "Using the `partition_pdf` function from the `unstructured` library, this function automatically identifies and extracts different types of content from your PDF.\n",
    "\n",
    "**Key Parameters:**\n",
    "- `infer_table_structure=True`: Reconstructs the underlying structure of tables.\n",
    "- `extract_images_in_pdf=True`: Extracts all images.\n",
    "- `extract_image_block_to_payload=True`: Stores extracted images directly as base64 strings within the element's metadata.\n",
    "- `chunking_strategy=\"by_title\"`: Groups related content under the same title, which is effective for textbooks.\n",
    "\n",
    "To choose different chunking strategy/parameters: https://docs.unstructured.io/open-source/core-functionality/chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ced8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "output_path = \"./content/\"\n",
    "file_path = output_path + 'YOUR_PDF_FILE.pdf'\n",
    "\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,\n",
    "    strategy=\"hi_res\", # The \"hi_res\" strategy is necessary if you want to infer tables\n",
    "    extract_images_in_pdf=True, \n",
    "\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],   \n",
    "    extract_image_block_to_payload=True, # If true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=10000,                  \n",
    "    combine_text_under_n_chars=2000,      \n",
    "    new_after_n_chars=6000,\n",
    "\n",
    "    extract_images_in_pdf=False, # Set to True if you want to extract images from the PDF seperate from base64 (not needed for RAG)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f661b",
   "metadata": {},
   "source": [
    "The output of the partitioner is a list of `CompositeElement` objects. Each `CompositeElement` contains a group of related smaller elements, making them easy to use together in a RAG pipeline. The contents of one chunk can be inspected below to understand the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84163d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].metadata.orig_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9d7df",
   "metadata": {},
   "source": [
    "A chunk can also be cast to a string to see its primary text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349a0e7",
   "metadata": {},
   "source": [
    "### 3b. Caching the Partitioned Data\n",
    "PDF partitioning can be time-consuming for large documents. To save time on future runs, the `chunks` object can be saved to a `.pkl` file. The **SAVE** cell only needs to be run once after the initial parse. Every subsequent session or if kernel needs to be restarted, the parsed  `chunks` can be loaded back in with the **LOAD** cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ae3bc",
   "metadata": {},
   "source": [
    "Run the cell below to **SAVE** the partitioned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"partitioned_output.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307bdc56",
   "metadata": {},
   "source": [
    "Run the cell below to **LOAD** the partitioned data on subsequent sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10818e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"partitioned_output.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58e4d2",
   "metadata": {},
   "source": [
    "## 4. Element Processing and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7f1a3",
   "metadata": {},
   "source": [
    "The `partition_pdf` function returns a list of `CompositeElement` objects. These elements now need to be processed and separated by type for the next steps.\n",
    "### 4a. Separating Elements by Type\n",
    "The partitioned elements are now separated into distinct lists of texts and tables for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    for elem in chunk.metadata.orig_elements:\n",
    "        if elem.to_dict()[\"type\"] == 'Table':\n",
    "            tables.append(elem)\n",
    "            chunk.metadata.orig_elements.remove(elem) # The table is removed from the original elements to avoid duplication\n",
    "    texts.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea6e8d",
   "metadata": {},
   "source": [
    "### 4b. Extracting Image Data\n",
    "This function iterates through all chunks and extracts the base64-encoded image data that unstructured stored in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f708b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51795d98",
   "metadata": {},
   "source": [
    "### 4c. Verifying Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d8e39",
   "metadata": {},
   "source": [
    "An individual table element can now be inspected. The `to_dict()` method provides a structured view of its content and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4249aa6",
   "metadata": {},
   "source": [
    "One of the extracted images can be displayed to confirm that the process worked correctly. The images are stored as base64 strings, which can be decoded and displayed directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d13387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "# Display the first extracted image to verify\n",
    "if images:\n",
    "    display_base64_image(images[0])\n",
    "else:\n",
    "    print(\"No images found in the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdaf20a",
   "metadata": {},
   "source": [
    "## 5. Element Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13805776",
   "metadata": {},
   "source": [
    "For the `MultiVectorRetriever` strategy to work, a summary of every element (text, table, and image) is needed. These summaries will be embedded and used for searching.\n",
    "\n",
    "* Texts and Tables: A fast, open-source model via Groq (`gemma2-9b-it` is a good choice) will be used to generate concise text summaries.\n",
    "* Images: A multi-modal model (like `gpt-4o-mini`) is needed to generate detailed descriptions of the images, paying special attention to diagrams and charts relevant to chemical engineering.\n",
    "\n",
    "### 5a. Generating Text and Table Summaries\n",
    "Groq is used for its and open-source models, which are ideal for summarizing a large number of text chunks and tables. A prompt is constructed to instruct the model to provide a concise summary. `Asynchronous I/O` is used to create a delay between API calls avoid exceeding the Groq Free Tier TPM limits.\n",
    "\n",
    "First, install dependency to integrate `LangChain` and `Groq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5fe0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603caf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import asyncio\n",
    "\n",
    "# Initialize the Groq model\n",
    "model = ChatGroq(temperature=0.2, model=\"gemma2-9b-it\")\n",
    "\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for a chemical engineering textbook. Give a concise summary of the table or text.\n",
    "Respond only with the summary. Do not start your message with \"Here is a summary\" or any other introductory text.\n",
    "Table or text chunk: {element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Define the summarization chain\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Async function with delay to not exceded TPM limit\n",
    "async def process_with_delay(elements, delay=2):\n",
    "    results = []\n",
    "    for i, element in enumerate(elements):\n",
    "        print(f\"Summarizing element {i+1}/{len(elements)}...\")\n",
    "        summary = summarize_chain.invoke(element)\n",
    "        results.append(summary)\n",
    "        if i < len(elements) - 1:\n",
    "            await asyncio.sleep(delay)\n",
    "    return results\n",
    "\n",
    "# Run the async summarization tasks for both texts and tables\n",
    "texts_summaries = await process_with_delay(texts, delay=2)\n",
    "tables_html = [table.metadata.text_as_html for table in tables]\n",
    "table_summaries = await process_with_delay(tables_html, delay=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47187421",
   "metadata": {},
   "source": [
    "### 5b. Caching Text and Table Summaries\n",
    "To avoid re-running this step (making more API calls), the generated summaries can be saved to a `.pkl` file similar to the `chunks`. The **SAVE** cell only needs to be run once after inital summaries are made. Every subsequent session or if kernel needs to be restarted, the summaries can be loaded back in with the **LOAD** cell.\n",
    "\n",
    "Run the cell below to **SAVE** the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"summaries.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"texts_summaries\": texts_summaries, \"table_summaries\": table_summaries}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d60b5f",
   "metadata": {},
   "source": [
    "Run the cell below to **LOAD** the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1cf5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"summaries.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    texts_summaries = data[\"texts_summaries\"]\n",
    "    table_summaries = data[\"table_summaries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f89e9f",
   "metadata": {},
   "source": [
    "### 5c. Generating Image Summaries\n",
    "For the images, a multi-modal model is required to understand and describe visual content like schematics and process flow diagrams (such as `gpt-4o-mini`).\n",
    "\n",
    "**Note on Content Filtering:** The Azure OpenAI service has a content safety filter. Some images, especially complex diagrams, might be flagged, causing the API call to fail. The code includes a `try...except` block to catch these errors, log the failed images, and continue the process. A delay between requests is also included to manage API TPM limits.\n",
    "\n",
    "First, install dependency to integrate `LangChain` and `OpenAI`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from openai import BadRequestError\n",
    "\n",
    "prompt_template_text = \"\"\"Describe the image in detail. The image is part of a textbook on material\n",
    "                         & energy balances and other introductory Chemical\n",
    "                         Engineering concepts. Be specific about graphs,\n",
    "                         such as enthalpy and entropy graphs, and diagrams\n",
    "                         that show chemical engineering processes.\n",
    "                      \"\"\"\n",
    "messages_template = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template_text},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages_template)\n",
    "\n",
    "# Initialize the Azure Chat model for multi-modal tasks\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\", \"2024-02-01\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    model=\"YOUR_MULTIMODAL_LLM_MODEL_NAME\",  # Replace with your model name (e.g., \"gpt-4o-mini\")\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# Define the image summarization chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "DELAY_BETWEEN_REQUESTS = 5\n",
    "image_summaries = []\n",
    "failed_image_indices = [] # Keeps track of images that were not able to be processed\n",
    "\n",
    "if 'images' in locals() and images:\n",
    "    for i, image_data in enumerate(images):\n",
    "        print(f\"Processing image {i+1} of {len(images)}...\")\n",
    "        try:\n",
    "            summary = chain.invoke({\"image\": image_data})\n",
    "            image_summaries.append(summary)\n",
    "\n",
    "        except BadRequestError as e:\n",
    "            # If a content filter error occurs, log it and continue\n",
    "            print(f\"--> Content filter triggered for image {i+1}. Skipping. Error: {e}\")\n",
    "            failed_image_indices.append(i)\n",
    "\n",
    "        finally:\n",
    "            # This 'finally' block ensures the delay happens even if an error occurs\n",
    "            if i < len(images) - 1:\n",
    "                print(f\"Waiting for {DELAY_BETWEEN_REQUESTS} seconds...\")\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "    print(\"\\nProcessing complete.\")\n",
    "    if failed_image_indices:\n",
    "        print(f\"The following image indices failed due to content filtering: {failed_image_indices}\")\n",
    "\n",
    "else:\n",
    "    print(\"The 'images' variable is not defined or is empty. Please populate it with image data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78363bfd",
   "metadata": {},
   "source": [
    "### 5d. Caching Image Summaries\n",
    "To avoid re-running this step (making more API calls, especially expensive for multi-modal LLMs), the generated summaries can be saved to a `.pkl` file similar to the `chunks`. The **SAVE** cell only needs to be run once after inital summaries are made. Every subsequent session or if kernel needs to be restarted, the summaries can be loaded back in with the **LOAD** cell.\n",
    "\n",
    "Run the cells below to **SAVE** the summaries and failed image indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"img_summaries.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"image_summaries\": image_summaries}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5457c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"failed_img_indices.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"failed_image_indices\": failed_image_indices}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27437fa",
   "metadata": {},
   "source": [
    "Run the cells below to **LOAD** the summaries and failed image indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24976867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"img_summaries.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    image_summaries = data[\"image_summaries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"failed_img_indices.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    failed_image_indices = data[\"failed_image_indices\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d234a21e",
   "metadata": {},
   "source": [
    "## 6. Building the Vector Store and Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ecf35",
   "metadata": {},
   "source": [
    "With all summaries generated, the retrieval system can now be built using a `MultiVectorRetriever`.\n",
    "* **Vector Store:** Contains the vector embeddings of the summaries using `ChromaDB`.\n",
    "* **Document Store:** Contains the original, full-sized elements (text chunks, table objects, and raw image data). An in-memory store is used here for simplicity.\n",
    "* **Link:** Each summary in the vector store is linked to its original document in the document store using a unique ID using an embedding model (such as `text-embedding-3-small`).\n",
    "\n",
    "When a query is made, the system searches the vector store for relevant summaries and retrieves the corresponding original documents from the document store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fe752",
   "metadata": {},
   "source": [
    "### 6a. Initializing the Vector and Document Stores\n",
    "First, install dependency to integrate `LangChain` and `ChromaDB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe82420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks (summaries)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal_rag\",\n",
    "    embedding_function=AzureOpenAIEmbeddings(\n",
    "        azure_deployment=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_EMBEDDING_API_KEY\"),\n",
    "        api_version=os.getenv(\"EMBEDDING_API_VERSION\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents (original elements)\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever, which will be populated in the next step\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cccf2b",
   "metadata": {},
   "source": [
    "### 6b. Populating the Stores\n",
    "For each element type (text, table, image), the process is:\n",
    "* Generate unique IDs for each original element.\n",
    "* Create Document objects for the summaries, adding the unique ID to the metadata.\n",
    "* Add the summary documents to the vectorstore.\n",
    "* Add the original elements to the docstore, using the same unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text summaries and link to original text chunks\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(texts_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add table summaries and link to original table objects\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries and link to original image data\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e2fcd9",
   "metadata": {},
   "source": [
    "### 6c.  Testing the Retriever\n",
    "A sample query can be run to see what the retriever returns. The output should be the original, full-content documents (not the summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17254ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve documents based on a query\n",
    "docs = retriever.invoke(\n",
    "    \"who are the authors of the paper?\"\n",
    ")\n",
    "\n",
    "# Display the retrieved documents\n",
    "for doc in docs:\n",
    "    if isinstance(doc, str):\n",
    "        # This is a base64 image\n",
    "        display_base64_image(doc)\n",
    "    else:\n",
    "        # This is a text or table element\n",
    "        print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631e826",
   "metadata": {},
   "source": [
    "## 7. RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bd718",
   "metadata": {},
   "source": [
    "All components are now assembled into a final, runnable chain using `LangChain Expression Language` (LCEL).\n",
    "The chain will:\n",
    "* Take a user's question.\n",
    "* Use the `retriever` to find relevant documents (text, tables, or images).\n",
    "* Use a `parse_docs` function to separate the retrieved documents by type.\n",
    "* Use a `build_prompt` function to construct a multi-modal prompt with the question and retrieved context.\n",
    "* Send this prompt to the `Azure` multi-modal LLM model to generate the final answer.\n",
    "\n",
    "**Two versions of the chain are created:** one for a direct answer (`chain`) and another that also returns the retrieved source documents (`chain_with_sources`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the Azure LLM from the environment variables set at the start of the notebook\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    ")\n",
    "\n",
    "def parse_docs(docs):\n",
    "    # Split retrieved documents into images and texts/tables\n",
    "    b64_images = []\n",
    "    text_docs = []\n",
    "    for doc in docs:\n",
    "        if isinstance(doc, str):\n",
    "            b64_images.append(doc)\n",
    "        elif hasattr(doc, 'page_content'):\n",
    "            text_docs.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": text_docs}\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    # Builds the multi-modal prompt for the LLM\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "    context_text = \"\"\n",
    "\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            # Check for page_content for Documents or text for other element types\n",
    "            content = getattr(text_element, 'page_content', getattr(text_element, 'text', ''))\n",
    "            context_text += content + \"\\n\\n\"\n",
    "\n",
    "    prompt_template = f\"\"\"Answer the question based only on the following context, which can include text, tables, and images.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {user_question}\n",
    "\"\"\"\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "    return ChatPromptTemplate.from_messages([HumanMessage(content=prompt_content)])\n",
    "\n",
    "# Define the common part of the chain to generate a response\n",
    "response_generator = RunnableLambda(build_prompt) | llm | StrOutputParser()\n",
    "\n",
    "# The RAG chain for a simple string response\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | response_generator\n",
    ")\n",
    "\n",
    "# The RAG chain that includes the source documents in the output\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(response=response_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0c7fe",
   "metadata": {},
   "source": [
    "## 8. Querying the RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22194a4",
   "metadata": {},
   "source": [
    "The complete system can now be tested with questions.\n",
    "\n",
    "**Example 1:** Factual Question about Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"What is a material balance?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9aa49",
   "metadata": {},
   "source": [
    "**Example 2:** Visual Question with Source Verification\n",
    "This question likely requires information from a diagram. The `chain_with_sources` is used to see both the LLM's answer and the context it used to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16af664",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_with_sources.invoke(\n",
    "    \"What does a distillation column look like?\"\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['response'])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Context Used:\\n\")\n",
    "\n",
    "# Print the text/table context\n",
    "for text in response['context']['texts']:\n",
    "    print(\"--- TEXT/TABLE CONTEXT ---\")\n",
    "    content = getattr(text, 'page_content', getattr(text, 'text', ''))\n",
    "    print(content)\n",
    "    if hasattr(text, 'metadata') and 'page_number' in text.metadata:\n",
    "        print(\"Page number: \", text.metadata.get('page_number'))\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Display the image context\n",
    "for image in response['context']['images']:\n",
    "    print(\"--- IMAGE CONTEXT ---\")\n",
    "    display_base64_image(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
